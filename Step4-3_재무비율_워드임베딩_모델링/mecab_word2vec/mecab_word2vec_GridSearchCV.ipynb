{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./mecab_word2vec_부도기사비율완전체.csv')\n",
    "data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "data.rename(columns={'운전자산총자본비율':'운전자본비율'}, inplace=True)\n",
    "data.fillna(0, inplace=True)\n",
    "# data.drop(['회사명', '회계년도', '폐지일자'], axis=1, inplace=True)\n",
    "input = data.iloc[:,[2,3,4,5,6,8]]\n",
    "target = data.iloc[:,9]\n",
    "\n",
    "columns = ['Accuracy', 'Roc_auc_score', 'Recall']\n",
    "비교테이블 = pd.DataFrame(columns = columns)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBClassifier\n",
    "##### 설명 참고 : https://wooono.tistory.com/97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best f1 : 0.9532664305391577\n",
      "best param :  {'gamma': 2, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "rs = RobustScaler()\n",
    "x_train = rs.fit_transform(x_train)\n",
    "x_test = rs.fit_transform(x_test)\n",
    "\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "xgb_param_grid = {'n_estimators' : [100, 200],\n",
    "                'learning_rate' : [0.01, 0.05, 0.1],\n",
    "                'max_depth' : [3, 5, 7],\n",
    "                'gamma' : [0, 1, 2]}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb, param_grid=xgb_param_grid, scoring='roc_auc', verbose=0, n_jobs=1)\n",
    "xgb_grid.fit(x_train, y_train)\n",
    "\n",
    "print(f'best f1 : {xgb_grid.best_score_}')\n",
    "print('best param : ', xgb_grid.best_params_)\n",
    "\n",
    "## 참고 : https://cjh34544.tistory.com/m/4\n",
    "## http://aispiration.com/model/model-python-xgboost-hyper.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best f1 : 0.9532664305391577\n",
      "best param :  {'gamma': 2, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "xgb_param_grid = {'n_estimators' : [100, 200],\n",
    "                'learning_rate' : [0.01, 0.05, 0.1],\n",
    "                'max_depth' : [3, 5, 7],\n",
    "                'gamma' : [0, 1, 2]}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb, param_grid=xgb_param_grid, scoring='roc_auc', verbose=0, n_jobs=1)\n",
    "xgb_grid.fit(x_train, y_train)\n",
    "\n",
    "print(f'best f1 : {xgb_grid.best_score_}')\n",
    "print('best param : ', xgb_grid.best_params_)\n",
    "\n",
    "## 참고 : https://cjh34544.tistory.com/m/4\n",
    "## http://aispiration.com/model/model-python-xgboost-hyper.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best f1 : 0.9572215663124755\n",
      "best param :  {'C': 0.1, 'penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dbswo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "25 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dbswo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\dbswo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\dbswo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\dbswo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.9382133         nan 0.94724518        nan 0.95722157\n",
      "        nan 0.95655254        nan 0.95657222]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "rs = RobustScaler()\n",
    "x_train = rs.fit_transform(x_train)\n",
    "x_test = rs.fit_transform(x_test)\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "lr_param_grid = {'C' : [0.001, 0.01, 0.1, 1, 10],\n",
    "                'penalty' : ['l1', 'l2']}\n",
    "\n",
    "lr_grid = GridSearchCV(lr, param_grid=lr_param_grid, scoring='roc_auc', verbose=0, n_jobs=1)\n",
    "lr_grid.fit(x_train, y_train)\n",
    "\n",
    "print(f'best f1 : {lr_grid.best_score_}')\n",
    "print('best param : ', lr_grid.best_params_)\n",
    "\n",
    "# 참고 : https://wikidocs.net/16594\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best f1 : 0.9561393152302244\n",
      "best param :  {'C': 0.1, 'penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dbswo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "25 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dbswo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\dbswo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\dbswo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\dbswo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.95181031        nan 0.95527351        nan 0.95613932\n",
      "        nan 0.95570641        nan 0.95570641]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "lr_param_grid = {'C' : [0.001, 0.01, 0.1, 1, 10],\n",
    "                'penalty' : ['l1', 'l2']}\n",
    "\n",
    "lr_grid = GridSearchCV(lr, param_grid=lr_param_grid, scoring='roc_auc', verbose=0, n_jobs=1)\n",
    "lr_grid.fit(x_train, y_train)\n",
    "\n",
    "print(f'best f1 : {lr_grid.best_score_}')\n",
    "print('best param : ', lr_grid.best_params_)\n",
    "\n",
    "# 참고 : https://wikidocs.net/16594"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best f1 : 0.9551160960251869\n",
      "best param :  {'max_depth': 3, 'min_samples_leaf': 16, 'min_samples_split': 8, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "rs = RobustScaler()\n",
    "x_train = rs.fit_transform(x_train)\n",
    "x_test = rs.fit_transform(x_test)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf_param_grid = {'n_estimators' : [100, 200],\n",
    "                'max_depth' : [3, 5, 7],\n",
    "                'min_samples_leaf' : [8, 12, 16],\n",
    "                'min_samples_split' : [8, 16, 20]}\n",
    "\n",
    "rf_grid = GridSearchCV(rf, param_grid=rf_param_grid, scoring='roc_auc', verbose=0, n_jobs=1)\n",
    "rf_grid.fit(x_train, y_train)\n",
    "\n",
    "print(f'best f1 : {rf_grid.best_score_}')\n",
    "print('best param : ', rf_grid.best_params_)\n",
    "\n",
    "## 참고 : https://techblog-history-younghunjo1.tistory.com/102\n",
    "## https://jaaamj.tistory.com/35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best f1 : 0.9551160960251869\n",
      "best param :  {'max_depth': 3, 'min_samples_leaf': 16, 'min_samples_split': 8, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf_param_grid = {'n_estimators' : [100, 200],\n",
    "                'max_depth' : [3, 5, 7],\n",
    "                'min_samples_leaf' : [8, 12, 16],\n",
    "                'min_samples_split' : [8, 16, 20]}\n",
    "\n",
    "rf_grid = GridSearchCV(rf, param_grid=rf_param_grid, scoring='roc_auc', verbose=0, n_jobs=1)\n",
    "rf_grid.fit(x_train, y_train)\n",
    "\n",
    "print(f'best f1 : {rf_grid.best_score_}')\n",
    "print('best param : ', rf_grid.best_params_)\n",
    "\n",
    "## 참고 : https://techblog-history-younghunjo1.tistory.com/102\n",
    "## https://jaaamj.tistory.com/35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best f1 : 0.9640889413616687\n",
      "best param :  {'C': 10, 'gamma': 0.01}\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "rs = RobustScaler()\n",
    "x_train = rs.fit_transform(x_train)\n",
    "x_test = rs.fit_transform(x_test)\n",
    "\n",
    "svc = SVC(random_state=42)\n",
    "\n",
    "svc_param_grid = {'C' : [0.001, 0.01, 0.1, 1, 10],\n",
    "                'gamma' : [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "svc_grid = GridSearchCV(svc, param_grid=svc_param_grid, scoring='roc_auc', verbose=0, n_jobs=1)\n",
    "svc_grid.fit(x_train, y_train)\n",
    "\n",
    "print(f'best f1 : {svc_grid.best_score_}')\n",
    "print('best param : ', svc_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best f1 : 0.8655745769382133\n",
      "best param :  {'C': 1, 'gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "svc = SVC(random_state=42)\n",
    "\n",
    "svc_param_grid = {'C' : [0.001, 0.01, 0.1, 1, 10],\n",
    "                'gamma' : [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "svc_grid = GridSearchCV(svc, param_grid=svc_param_grid, scoring='roc_auc', verbose=0, n_jobs=1)\n",
    "svc_grid.fit(x_train, y_train)\n",
    "\n",
    "print(f'best f1 : {svc_grid.best_score_}')\n",
    "print('best param : ', svc_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoostClassifier는 파라미터 조정이 성능에 크게 영향을 미치지 않는다는 말이 많아 일단 생략함\n",
    "##### https://velog.io/@jus6886/Catboost\n",
    "##### https://undeadkwandoll.tistory.com/61\n",
    "##### https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002698429\n",
    "#### CatBoost 설명\n",
    "##### https://dailyheumsi.tistory.com/136\n",
    "##### https://techblog-history-younghunjo1.tistory.com/199\n",
    "##### https://heeya-stupidbutstudying.tistory.com/43?category=950711\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8722527472527473\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "rs = RobustScaler()\n",
    "x_train = rs.fit_transform(x_train)\n",
    "x_test = rs.fit_transform(x_test)\n",
    "\n",
    "cat = CatBoostClassifier(random_state=42, verbose=0)\n",
    "cat.fit(x_train, y_train)\n",
    "pred = cat.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "recall = recall_score(y_test, pred)\n",
    "precision = precision_score(y_test, pred)\n",
    "roc = roc_auc_score(y_test, pred)\n",
    "\n",
    "print(roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lightGBM은 10000개 이하의 데이터에 overfitting하기 쉬워서 사용 x\n",
    "##### https://nurilee.com/2020/04/03/lightgbm-definition-parameter-tuning/\n",
    "##### https://mac-user-guide.tistory.com/79"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting\n",
    "##### 코드 참고 : https://eunki.tistory.com/60\n",
    "##### https://nonmeyet.tistory.com/entry/Python-Voting-Classifiers%EB%8B%A4%EC%88%98%EA%B2%B0-%EB%B6%84%EB%A5%98%EC%9D%98-%EC%A0%95%EC%9D%98%EC%99%80-%EA%B5%AC%ED%98%84\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 모델 4개 사용한 hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9649555774925962 recall : 0.985285462036492, precision : 0.9732558139534884, f1 : 0.9792336940625913\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "tl = TomekLinks()\n",
    "x_train, y_train = tl.fit_resample(x_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(gamma=0, learning_rate=0.1, max_depth=7, n_estimators=200)\n",
    "lr = LogisticRegression(C=0.1, penalty='l2')\n",
    "rf = RandomForestClassifier(verbose=0)\n",
    "cat = CatBoostClassifier(verbose=0)\n",
    "\n",
    "model = VotingClassifier(estimators=[('xgb', xgb), ('lr', lr), ('rf', rf), ('cat', cat)], weights=[2, 1, 2, 2], voting='hard')\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "recall = recall_score(y_test, pred)\n",
    "precision = precision_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "\n",
    "print('accuracy : {0} recall : {1}, precision : {2}, f1 : {3}'.format(acc, recall, precision, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 4개 사용한 soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9684106614017769 recall : 0.9882283696291937, precision : 0.9744631456761462, f1 : 0.9812974868497955\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "tl = TomekLinks()\n",
    "x_train, y_train = tl.fit_resample(x_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(gamma=1, learning_rate=0.1, max_depth=5, n_estimators=200)\n",
    "lr = LogisticRegression(C=0.1, penalty='l2')\n",
    "rf = RandomForestClassifier(verbose=0)\n",
    "cat = CatBoostClassifier(verbose=0)\n",
    "\n",
    "model = VotingClassifier(estimators=[('xgb', xgb), ('lr', lr), ('rf', rf), ('cat', cat)], weights=[2, 1, 2, 2], voting='soft')\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "recall = recall_score(y_test, pred)\n",
    "precision = precision_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "\n",
    "print('accuracy : {0} recall : {1}, precision : {2}, f1 : {3}'.format(acc, recall, precision, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 3개 사용한 hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9693978282329714 recall : 0.9888169511477339, precision : 0.9750435287289612, f1 : 0.9818819403857393\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "tl = TomekLinks()\n",
    "x_train, y_train = tl.fit_resample(x_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(gamma=1, learning_rate=0.1, max_depth=5, n_estimators=200)\n",
    "lr = LogisticRegression(C=0.1, penalty='l2')\n",
    "rf = RandomForestClassifier(verbose=0)\n",
    "cat = CatBoostClassifier(verbose=0)\n",
    "\n",
    "model = VotingClassifier(estimators=[('xgb', xgb), ('rf', rf), ('cat', cat)], weights=[1, 1, 1], voting='hard')\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "recall = recall_score(y_test, pred)\n",
    "precision = precision_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "roc = roc_auc_score(y_test, pred)\n",
    "\n",
    "print('accuracy : {0} recall : {1}, precision : {2}, f1 : {3}'.format(acc, recall, precision, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 3개 사용한 soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9689042448173741 recall : 0.987051206592113, precision : 0.9761350407450524, f1 : 0.9815627743634768\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "tl = TomekLinks()\n",
    "x_train, y_train = tl.fit_resample(x_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(gamma=1, learning_rate=0.1, max_depth=5, n_estimators=200)\n",
    "rf = RandomForestClassifier(verbose=0)\n",
    "cat = CatBoostClassifier(verbose=0)\n",
    "\n",
    "model = VotingClassifier(estimators=[('xgb', xgb), ('rf', rf), ('cat', cat)], weights=[1, 1, 1], voting='soft')\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "recall = recall_score(y_test, pred)\n",
    "precision = precision_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "roc = roc_auc_score(y_test, pred)\n",
    "\n",
    "print('accuracy : {0} recall : {1}, precision : {2}, f1 : {3}'.format(acc, recall, precision, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 2개 사용한 hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9684106614017769 recall : 0.9811653914067098, precision : 0.9811653914067098, f1 : 0.9811653914067098\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "tl = TomekLinks()\n",
    "x_train, y_train = tl.fit_resample(x_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(gamma=1, learning_rate=0.1, max_depth=5, n_estimators=200)\n",
    "cat = CatBoostClassifier(verbose=0)\n",
    "\n",
    "model = VotingClassifier(estimators=[('xgb', xgb), ('cat', cat)], voting='hard')\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "recall = recall_score(y_test, pred)\n",
    "precision = precision_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "\n",
    "print('accuracy : {0} recall : {1}, precision : {2}, f1 : {3}'.format(acc, recall, precision, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 2개 사용한 soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9684106614017769 recall : 0.985285462036492, precision : 0.9772329246935202, f1 : 0.981242672919109\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "tl = TomekLinks()\n",
    "x_train, y_train = tl.fit_resample(x_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(gamma=1, learning_rate=0.1, max_depth=5, n_estimators=200)\n",
    "cat = CatBoostClassifier(verbose=0)\n",
    "\n",
    "model = VotingClassifier(estimators=[('xgb', xgb), ('cat', cat)], voting='soft')\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "recall = recall_score(y_test, pred)\n",
    "precision = precision_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "\n",
    "print('accuracy : {0} recall : {1}, precision : {2}, f1 : {3}'.format(acc, recall, precision, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 2개 사용한 soft voting (성능이 제일 좋은 catboost에 가중치를  조금 더 줬을 경우, 가중치 3이상은 결과 변화 x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9679170779861797 recall : 0.9846968805179518, precision : 0.9772196261682243, f1 : 0.9809440046907066\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "tl = TomekLinks()\n",
    "x_train, y_train = tl.fit_resample(x_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(gamma=1, learning_rate=0.1, max_depth=5, n_estimators=200)\n",
    "cat = CatBoostClassifier(verbose=0)\n",
    "\n",
    "model = VotingClassifier(estimators=[('xgb', xgb), ('cat', cat)], weights=[1, 2], voting='soft')\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "recall = recall_score(y_test, pred)\n",
    "precision = precision_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "\n",
    "print('accuracy : {0} recall : {1}, precision : {2}, f1 : {3}'.format(acc, recall, precision, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평가지표로 f1 score를 쓰는 이유\n",
    "##### https://towardsdatascience.com/read-this-before-using-roc-auc-as-a-metric-c84c2d5af621\n",
    "##### https://stackoverflow.com/questions/44172162/f1-score-vs-roc-auc\n",
    "##### https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting Model 비교\n",
    "##### https://medium.com/@divyagera2402/boosting-algorithms-adaboost-gradient-boosting-xgb-light-gbm-and-catboost-e7d2dbc4e4ca\n",
    "##### http://dmqm.korea.ac.kr/activity/seminar/323\n",
    "##### https://hyunlee103.tistory.com/25\n",
    "##### https://neptune.ai/blog/when-to-choose-catboost-over-xgboost-or-lightgbm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 라벨인코딩 vs 원핫인코딩\n",
    "##### https://wyatt37.tistory.com/11\n",
    "##### https://hye-z.tistory.com/16?category=501972\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한 것요약\n",
    "##### 원핫인코딩은 차원을 늘려 과적합되기 쉽다. 물론 규제로 어느정도 커버할 수 있으나 트리계열에서는 해당 변수가 아예 제외되는 문제점을 가진다. 차원의 저주 등등\n",
    "##### catboost는 파라미터 수정을 하지 않아도 효과가 나쁘지 않다\n",
    "##### lightgbm은 데이터의 수가 너무 적어 사용할 수 없다\n",
    "##### ROC가 F1보다 불균형 데이터 셋에 대해 관대한? 경향이 있어서 불균형 데이터 셋에는 F1을 평가지표로 사용한다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25a19fbe0a9132dfb9279d48d161753c6352f8f9478c2e74383d340069b907c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
